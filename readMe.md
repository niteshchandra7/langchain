# ğŸš€ Learn LangChain  

## ğŸ“Œ Prompt Templates  

Prompt templates help translate **user input and parameters** into clear instructions for a language model.  
They **guide the modelâ€™s response**, providing structure so the output is relevant and coherent.  

- A **Prompt Template** takes a dictionary as input.  
- Each **key** in the dictionary represents a variable in the template.  
- The template outputs a **PromptValue**, which can be:  
  - Passed to an LLM or ChatModel  
  - Cast into a **string** or a **list of messages**  

ğŸ‘‰ The reason `PromptValue` exists is to make switching between **strings** and **messages** easy.  

---

## ğŸ“ String Prompt Templates  

### uv add langchain langchain-openai langchain-tavily python-dotenv isort black
```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template(
    "Tell me a joke about {topic}"
)

# Using the template
prompt_template.invoke({"topic": "cats"})
```

## ğŸ”— LangChain Chain Workflow  

The step-by-step process of how LangChain handles a query:  

1. **ğŸ“ User Query** â†’ Raw question or request from the user  
2. **ğŸ“Œ Prompt Template** â†’ Formats the query into a structured prompt  
3. **ğŸ¤– Language Model** â†’ Generates a response  
4. **ğŸ§© Output Parser** â†’ Parses LLM output into structured data  
5. **ğŸŒ External API / Tool Call** â†’ Calls external services if needed  
6. **ğŸ” Final LLM Call** â†’ Processes API responses  

## ğŸ“Œ AI Agents - ReAct Architecture

1. ** LangChain ReAct Agent** â†’ ReAct Prompt
2. ** Tool Calling Agent** â†’ Function Calling
3. ** LangGraph ReAct Agent** â†’ Function Calling


